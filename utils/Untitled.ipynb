{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fd7ab-c408-4614-bbc9-44286a533846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydicom\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from typing import Dict, List, Tuple\n",
    "from skimage.transform import resize\n",
    "\n",
    "class DatasetManager:\n",
    "    def __init__(self, data_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize the DatasetManager with a directory path.\n",
    "\n",
    "        Parameters:\n",
    "        data_dir (str): Path to the dataset directory containing DICOM files.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.data = {}\n",
    "        self.metadata = {}\n",
    "        self.normalizer = None\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Recursively load DICOM files from directories into a dictionary where each key is a class name \n",
    "        and each value is a list of DICOM image arrays.\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            class_name = os.path.basename(root)  # Assume the class name is the name of the deepest directory\n",
    "            if not files:  # Skip empty directories\n",
    "                continue\n",
    "            if class_name not in self.data:\n",
    "                self.data[class_name] = []\n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.dcm'):\n",
    "                    file_path = os.path.join(root, file_name)\n",
    "                    self.data[class_name].append(self._load_file(file_path))\n",
    "        self.generate_metadata()\n",
    "\n",
    "    def generate_metadata(self) -> None:\n",
    "        \"\"\"\n",
    "        Generate metadata such as number of samples per class, total samples, and class balance.\n",
    "        \"\"\"\n",
    "        class_counts = {class_name: len(files) for class_name, files in self.data.items()}\n",
    "        self.metadata['class_counts'] = class_counts\n",
    "        self.metadata['total_samples'] = sum(class_counts.values())\n",
    "        self.metadata['class_balance'] = {\n",
    "            class_name: count / self.metadata['total_samples']\n",
    "            for class_name, count in class_counts.items()\n",
    "        }\n",
    "\n",
    "    def check_data_quality(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Perform data quality checks and return any issues found.\n",
    "\n",
    "        Returns:\n",
    "        Dict[str, List[str]]: Dictionary containing quality issues per class.\n",
    "        \"\"\"\n",
    "        quality_issues = {}\n",
    "        for class_name, files in self.data.items():\n",
    "            issues = []\n",
    "            for i, file_data in enumerate(files):\n",
    "                if not self._is_valid(file_data):\n",
    "                    issues.append(f\"File {i} in class {class_name} is corrupted\")\n",
    "            quality_issues[class_name] = issues\n",
    "        self.metadata['quality_issues'] = quality_issues\n",
    "        return quality_issues\n",
    "\n",
    "    def preprocess_data(self, method: str = 'standard', target_shape: Tuple[int, int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Preprocess the data using the specified normalization method.\n",
    "\n",
    "        Parameters:\n",
    "        method (str): Normalization method ('standard' or 'minmax').\n",
    "        target_shape (Tuple[int, int], optional): Target shape to resize images. If None, use the shape of the first image in each class.\n",
    "        \"\"\"\n",
    "        if method == 'standard':\n",
    "            self.normalizer = StandardScaler()\n",
    "        elif method == 'minmax':\n",
    "            self.normalizer = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        for class_name, files in self.data.items():\n",
    "            if not files:  # Skip empty lists\n",
    "                print(f\"No data available for class '{class_name}'. Skipping normalization.\")\n",
    "                continue\n",
    "        \n",
    "            if target_shape is None:\n",
    "                target_shape = files[0].shape\n",
    "        \n",
    "            # Resize each file to the target shape if necessary\n",
    "            resized_files = []\n",
    "            for file in files:\n",
    "                if file.shape != target_shape:\n",
    "                    resized_file = resize(file, target_shape, mode='reflect', anti_aliasing=True)\n",
    "                else:\n",
    "                    resized_file = file\n",
    "                resized_files.append(resized_file.flatten())\n",
    "\n",
    "            flattened = np.array(resized_files)\n",
    "        \n",
    "            if len(flattened.shape) == 1:\n",
    "                # If there is only one sample, reshape it to a 2D array\n",
    "                flattened = flattened.reshape(1, -1)\n",
    "        \n",
    "            # Normalize the data\n",
    "            normalized = self.normalizer.fit_transform(flattened)\n",
    "        \n",
    "            # Reshape back to the target image shape\n",
    "            self.data[class_name] = [norm.reshape(target_shape) for norm in normalized]\n",
    "\n",
    "\n",
    "    def split_data(self, test_size: float = 0.2) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Split the data into training and test sets.\n",
    "\n",
    "        Parameters:\n",
    "        test_size (float): Proportion of data to use for testing.\n",
    "\n",
    "        Returns:\n",
    "        Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]: Training and test datasets.\n",
    "        \"\"\"\n",
    "        train_data = {}\n",
    "        test_data = {}\n",
    "        for class_name, files in self.data.items():\n",
    "            train, test = train_test_split(files, test_size=test_size, random_state=42)\n",
    "            train_data[class_name] = train\n",
    "            test_data[class_name] = test\n",
    "        return train_data, test_data\n",
    "\n",
    "    def visualize_data_distribution(self) -> None:\n",
    "        \"\"\"\n",
    "        Visualize the distribution of classes in the dataset.\n",
    "        \"\"\"\n",
    "        class_counts = self.metadata.get('class_counts', {})\n",
    "        sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "        plt.title('Class Distribution')\n",
    "        plt.xlabel('Class')\n",
    "        plt.ylabel('Number of Samples')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_sample(self, class_name: str, index: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Visualize a single DICOM data sample from a specified class.\n",
    "\n",
    "        Parameters:\n",
    "        class_name (str): The class from which to visualize a sample.\n",
    "        index (int): The index of the sample within the class.\n",
    "        \"\"\"\n",
    "        if class_name in self.data:\n",
    "            sample = self.data[class_name][index]\n",
    "            plt.imshow(sample, cmap='gray')\n",
    "            plt.title(f\"Class: {class_name}, Index: {index}\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Class '{class_name}' not found.\")\n",
    "\n",
    "    def convert_to_h5(self, output_file: str) -> None:\n",
    "        \"\"\"\n",
    "        Convert the dataset to HDF5 format.\n",
    "\n",
    "        Parameters:\n",
    "        output_file (str): Path to save the HDF5 file.\n",
    "        \"\"\"\n",
    "        with h5py.File(output_file, 'w') as h5f:\n",
    "            for class_name, files in self.data.items():\n",
    "                group = h5f.create_group(class_name)\n",
    "                for i, file_data in enumerate(files):\n",
    "                    group.create_dataset(f'data_{i}', data=file_data)\n",
    "\n",
    "    def extract_dicom_metadata(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extract metadata from all DICOM files.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: List of metadata dictionaries for each DICOM file.\n",
    "        \"\"\"\n",
    "        metadata_list = []\n",
    "        for class_name, files in self.data.items():\n",
    "            for i, file_data in enumerate(files):\n",
    "                metadata = {\n",
    "                    \"Class Name\": class_name,\n",
    "                    \"File Index\": i,\n",
    "                    \"Patient ID\": file_data.PatientID,\n",
    "                    \"Patient Name\": file_data.PatientName,\n",
    "                    \"Study Date\": file_data.StudyDate,\n",
    "                    \"Modality\": file_data.Modality,\n",
    "                    \"Institution Name\": file_data.InstitutionName,\n",
    "                    \"Body Part Examined\": file_data.BodyPartExamined,\n",
    "                    \"Study Description\": file_data.StudyDescription,\n",
    "                    \"Series Description\": file_data.SeriesDescription,\n",
    "                }\n",
    "                metadata_list.append(metadata)\n",
    "        self.metadata['dicom_metadata'] = metadata_list\n",
    "        return metadata_list\n",
    "\n",
    "    def save_metadata_to_excel(self, output_file: str) -> None:\n",
    "        \"\"\"\n",
    "        Save DICOM metadata to an Excel file.\n",
    "\n",
    "        Parameters:\n",
    "        output_file (str): Path to the output Excel file.\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        if 'dicom_metadata' in self.metadata:\n",
    "            df = pd.DataFrame(self.metadata['dicom_metadata'])\n",
    "            df.to_excel(output_file, index=False)\n",
    "\n",
    "    def _load_file(self, file_path: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Load a DICOM file.\n",
    "\n",
    "        Parameters:\n",
    "        file_path (str): Path to the DICOM file.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: DICOM image data as a NumPy array, or an empty array if loading fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ds = pydicom.dcmread(file_path)\n",
    "            return ds.pixel_array\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file_path}: {e}\")\n",
    "            return np.array([])  # Return an empty array if loading fails\n",
    "\n",
    "\n",
    "    def _is_valid(self, file_data: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the DICOM data is valid (e.g., not corrupted).\n",
    "\n",
    "        Parameters:\n",
    "        file_data (np.ndarray): Data to check.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the data is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        return file_data is not None and file_data.size > 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage of the DatasetManager class with DICOM files\n",
    "    data_manager = DatasetManager('../dataset/TCGA-KIRP')\n",
    "    data_manager.load_data()\n",
    "\n",
    "    # Generate metadata and check data quality\n",
    "    data_manager.generate_metadata()\n",
    "    print(\"Metadata:\", data_manager.metadata)\n",
    "    quality_issues = data_manager.check_data_quality()\n",
    "    print(\"Quality Issues:\", quality_issues)\n",
    "\n",
    "    # Preprocess data and visualize it\n",
    "    data_manager.preprocess_data(method='minmax')\n",
    "    data_manager.visualize_data_distribution()\n",
    "    data_manager.visualize_sample(class_name='01', index=0)\n",
    "\n",
    "    # Split the data and save to HDF5 format\n",
    "    train_data, test_data = data_manager.split_data(test_size=0.3)\n",
    "    data_manager.convert_to_h5('/path/to/output_file.h5')\n",
    "\n",
    "    # Extract and save DICOM metadata to Excel\n",
    "    dicom_metadata = data_manager.extract_dicom_metadata()\n",
    "    data_manager.save_metadata_to_excel('dicom_metadata.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959240d4-11f7-4c60-80df-149b829b919f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
